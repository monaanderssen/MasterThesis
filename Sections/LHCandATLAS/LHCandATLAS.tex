\chapter{LHC and ATLAS}
\label{sec:LHCandATLAS}
The data used in this thesis comes from the ATLAS experiment at the LHC. In this chapter, we briefly introduce the experiment and the accelerator as well as the computing infrastructure used to analyse the data. We will also get a small incite into the organization behind the experiment and the accelerator, namely CERN. 

\input{Sections/LHCandATLAS/cern}

\input{Sections/LHCandATLAS/LHC}

\input{Sections/LHCandATLAS/ATLAS}



\section{Computing infrastructure}
The results obtained in this thesis have been very demanding when it comes to computing power. The reason for this is that the analyses consists of several searches using BDT and deep learning methods that require both training and optimization by using large data sets. BDT and deep learning require a lot of CPU-power and memory because of the size of the data. It has not been possible to run the various codes on a regular computer because of limited number of CPU's and memory. Because of these problems we were granted access to the Experimental Infrastructure for Exploration of Exascale Computing at Simula Research Laboratory: financed by the research council of Norway and made available to researchers. This is a computer with two sockets with 8 cores/CPU's, which again have two threads. This gives us in all 32 virtual CPU's (16 physical) because of hyper-threading in each CPU. It also has 60 GiB\footnote{GiB is Gibibyte instead of regular gigabyte and is simply a unit byte for digital information and means 2 to the power of 10 (kiB), 20 (MiB), 30 (GiB), 40 (TiB) and 50 (PiB).} memory, which have been crucial to handle the data used in the Machine Learning analysis. With this setup, the import of the data, training and testing have taken approximately 12-13 days, where 7 of these are just for importing the data. All together we have trained and tested 72 ML models (36 BDTs and 36 NNs) and the data sets we have been working on have been massive (almost 200 GB). Because of the huge amount of data, we need that the computer can handle this while training which is why we need the extra memory.

At the later stage of this thesis we made use of a special server that belongs to the ATLAS High Energy Particle Physics (HEPP) group at UiO. It is a Supermicro Ultra Server with both GPU's and CPU's, but we have only taken advantage of the CPU's in this thesis. This server is a much more powerful computer than the one from Simula. It has two sockets with 128 cores/CPU's, which also have two threads in each CPU. This gives a total of 256 virtual CPU's. It also has 2 TiB memory, which has resulted in that we could import the data in parallel and be done in around 3 days instead of a week. We have been able to train around 18-20 ML models at the same time instead of 1-2 which was the maximum for the Simula server. 

In this chapter we have introduced the ATLAS detector and the LHC, which collected 13 TeV data between 2015 and 2018, corresponding to 139 fb$^{-1}$. The data analysis behind the searches for Supersymmetry and Dark Matter to be presented in this thesis make use of both traditional and ML-based algorithms, and necessitate a special computing infrastructure made of powerful CPU's, and GPU's.